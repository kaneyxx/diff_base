# LoRA Training Configuration

training:
  method: "lora"

  # LoRA Parameters
  lora:
    rank: 32
    alpha: 32
    dropout: 0.0
    target_modules:
      - "to_q"
      - "to_k"
      - "to_v"
      - "to_out.0"
      - "proj_in"
      - "proj_out"
      - "ff.net.0.proj"
      - "ff.net.2"
    train_text_encoder: false
    text_encoder_lr: 5.0e-5
    text_encoder_target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "out_proj"

  # Optimizer
  optimizer:
    type: "adamw"
    lr: 1.0e-4
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1.0e-8

  # Learning Rate Scheduler
  lr_scheduler:
    type: "cosine"
    warmup_steps: 500
    min_lr_ratio: 0.1

  # Loss Configuration
  loss:
    type: "mse"
    snr_gamma: 5.0  # Min-SNR weighting, null to disable

  # Training Settings
  batch_size: 4
  gradient_accumulation: 2
  epochs: 100
  max_grad_norm: 1.0
  save_every_n_epochs: 10
