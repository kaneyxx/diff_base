# Full Fine-tuning Configuration

training:
  method: "full_finetune"

  # Optimizer
  optimizer:
    type: "adamw"
    lr: 1.0e-5
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1.0e-8

  # Learning Rate Scheduler
  lr_scheduler:
    type: "constant"
    warmup_steps: 1000

  # Loss Configuration
  loss:
    type: "mse"
    snr_gamma: 5.0

  # Training Settings
  batch_size: 2
  gradient_accumulation: 4
  epochs: 50
  max_grad_norm: 1.0
  save_every_n_epochs: 5
