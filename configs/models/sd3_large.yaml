# SD3.5-Large Model Configuration
# ~8B parameter model with triple text encoders (CLIP-L + OpenCLIP-G + T5-XXL)
# Uses MM-DiT (Multimodal Diffusion Transformer) architecture

model:
  type: "sd3"
  variant: "large"

  # Transformer (MM-DiT) Configuration
  transformer:
    depth: 38
    hidden_size: 2432              # 64 * 38
    num_heads: 38                  # One head per layer
    patch_size: 2                  # Converts latent to patches
    in_channels: 16                # VAE latent channels
    pos_embed_max_size: 192        # Max patches per dimension
    qk_norm: true                  # QK normalization for stability
    context_dim: 4096              # T5-XXL hidden size
    pooled_projection_dim: 2048    # CLIP-L (768) + CLIP-G (1280)

  # VAE Configuration (16 channels, like FLUX)
  vae:
    in_channels: 3
    out_channels: 3
    latent_channels: 16            # SD3 uses 16 channels
    block_out_channels: [128, 256, 512, 512]
    layers_per_block: 2
    scaling_factor: 1.5305         # SD3-specific
    shift_factor: 0.0609           # SD3-specific
    use_quant_conv: true

  # Triple Text Encoder Configuration
  text_encoder:
    type: "triple"
    clip_l:
      model_id: "openai/clip-vit-large-patch14"
      hidden_size: 768
      num_layers: 12
      max_length: 77
    clip_g:
      model_id: "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
      hidden_size: 1280
      num_layers: 32
      max_length: 77
    t5:
      model_id: "google/t5-v1_1-xxl"
      hidden_size: 4096
      num_layers: 24
      max_length: 512

  # Scheduler Configuration (Flow Matching)
  scheduler:
    type: "flow_matching_euler"
    num_train_timesteps: 1000
    shift: 3.0
    base_shift: 0.5
    max_shift: 1.15
